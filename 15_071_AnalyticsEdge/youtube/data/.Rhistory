setwd("/Users/yukako/WorkSpace/LearnMLDM/15_071_AnalyticsEdge/youtube/")
USvideo_raw <- read.csv("data/USvideos.csv", encoding='utf-8')
INvideo_raw <- read.csv("data/INvideos.csv", encoding='utf-8')
JPvideo_raw <- read.csv("data/JPvideos.csv", encoding='utf-8')
USvideo <- USvideo_raw
dim(USvideo)
head(USvideo); str(USvideo);
###############################
# Data Cleaning
###############################
colnames(USvideo)
# 「コメントdisable、評価disable、エラーでremoved」じゃないデータを抽出
USvideo <- subset(USvideo,
!(USvideo[,13]=="True" | USvideo[,14]=="True" | USvideo[,15]=="True"))
dim(USvideo)
head(USvideo)
USvideo[,2] <- as.POSIXct(as.character(USvideo[,2]), format="%y.%d.%m")# as.DateかPOSIXctか
USvideo[,6] <- as.POSIXct(USvideo[,6], format="%Y-%m-%dT%H:%M:%S.000Z")
class(USvideo[,2])
class(USvideo[,6])
class(USvideo$publish_time)
# takes out "\\n"(=改行コード)など
USvideo$description <- gsub("\\\\n", " ", USvideo, 16)
USvideo$description <- gsub("http[^[:blank:]]+", "", USvideo$description)
USvideo$description <- gsub("www[^[:blank:]]+", "", USvideo$description)
USvideo$description <- gsub('[[:digit:]]+', "", USvideo$description)
USvideo$description <- gsub("[[:punct:]]+", "", USvideo$description)
USvideo$description <- gsub("\\s+"," ", USvideo$description)
# θのとりうる値
theta <- seq(0, 1, 0.1)
# 事前分布
prior <- c(0.01, 0.07, 0.1, 0.12, 0.13, 0.14, 0.13, 0.12, 0.1, 0.07, 0.01)
# データ（5回中4回表が出た）
data <- list(n = 5, s = 4)
data
# 尤度
likelihood <- dbinom(data$s, size = data$n, prob = theta)
likelihood
dbinom
data$s
data$n
theta
plot(likelihood)
dbinom(10, size = 3, prob = c(0.1, 0.1))
plot(dbinom(10, size = 3, prob = c(0.1, 0.1)))
plot(dbinom(10, size = 3, prob = c(0.5, 0.1)))
?dbinom
plot(dbinom(6, size = 10, prob = theta))
a <- dbinom(6, size = 10, prob = theta)
plot(a)
a
# 尤度
likelihood <- dbinom(data$s, size = data$n, prob = theta)
plot(likelihood)
likelihood
theta
dbinom(3, 10)
dbinom(3, 10, prob=0.1)
dbinom(4, 5, prob=0.5)
dbinom(4, 5, prob=0.5)
dbinom(4, 5, prob=0.5)
dbinom(4, 5, prob=0.5)*1000 / 1000
USvideo <- read.csv("USvideo_pd.csv")
library(ggplot2)
library(tm)
library(SnowballC)
library(rpart)
library(rpart.plot)
library("wordcloud")
library("RColorBrewer")
library(RMeCab)
options(scipen=999)
setwd("/Users/yukako/WorkSpace/ML/15_071_AnalyticsEdge/Youtube/data")
USvideo <- read.csv("USvideo_pd.csv")
JPvideo <- read.csv("JPvideo_pd.csv")
INvideo <- read.csv("INvideo_pd.csv")
video <- JPvideo
# ------------
# INとUS用の関数 : コーパスを作る
# ------------
makeCorpus <- function(t){
removeTerms <- c(stopwords("english"), "youtube", "video", "channel", "that",
"show", "watch", "can", "make", "us", "use", "subscribe",
"facebook", "twitter", "instagram", "•", "youtub", "–",
"2017", "2018", "will")
t <- gsub("–", "", t)
t <- gsub("video", "", t)
t <- gsub("【", "", t)
t <- gsub("】", "", t)
t <- gsub("subscribe", "", t)
corpus = Corpus(VectorSource(t))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, removeTerms) # ほかあれば
corpus = tm_map(corpus, stemDocument)
return (corpus)
}
# ------------
# JP用の関数 : 名詞と動詞だけのリスト作成
# ------------
make_nounverb_JP <- function(t){
# for(i in c(length(t))){
#   t[i] <- gsub("\"", "", t[i])
#   t[i] <- gsub("】", "", t[i])
#   t[i] <- gsub("【", "", t[i])
#   t[i] <- gsub("『", "", t[i])
#   t[i] <- gsub("』", "", t[i])
#   t[i] <- gsub("「", "", t[i])
#   t[i] <- gsub("」", "", t[i])
#   t[i] <- gsub("①", "", t[i])
#   t[i] <- gsub("②", "", t[i])
#   t[i] <- gsub("≪", "", t[i])
#   t[i] <- gsub("≫", "", t[i])
#   t[i] <- gsub("→", "", t[i])
#   t[i] <- gsub("年", "", t[i])
#   t[i] <- gsub("月", "", t[i])
#   t[i] <- gsub("日", "", t[i])
#   t[i] <- gsub("！\"", "", t[i])
#   t[i] <- gsub("\"", "", t[i])
#   t[i] <- gsub("〜", "", t[i])
# }
write.table(t, file="JPtext.txt", row.names=F, col.names=F)
JPtextFreq <- RMeCabFreq("JPtext.txt")
JPtext_nv <- JPtextFreq[JPtextFreq$Info1 == "名詞" | JPtextFreq$Info1 == "動詞",]
# いらなそうなやつを調べる
table(JPtext_nv$Info1, JPtext_nv$Info2)
subset(JPtext_nv, JPtext_nv$Info1 == "動詞" & JPtext_nv$Info2 == "接尾")
# "がる、させる、す、せる、られる、れる" を削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "動詞" & JPtext_nv$Info2 == "接尾"),]
# "こちら、あっち、あいつ、彼、みなさん"などを削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "名詞" & JPtext_nv$Info2 == "代名詞"),]
# 数字を削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "名詞" & JPtext_nv$Info2 == "数"),]
# 頻出単語をチラ見
JPtext_nv[order(JPtext_nv$Freq, decreasing=T)[1:20],]
# 頻出単語にゴミが混ざってるので手動で削除
removeTerms <- c(removeTerms, "動画", "チャンネル", "登録", "\"", "nn",
"する", "いる", "ある", "なる", "こと", "videos", "さん", "の",
"\"\"", "再生", "リスト", "nnn", "お願い", "！", "゙",
"みる", "〜", "—", "／", "「", "」", "2018", "2017",
"フェイスブック", "ツイッター", "インスタグラム")
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Term %in% removeTerms),]
# "aa"とか"ld"とか"`"とか無意味な単語とか記号を削除
JPtext_nv <- JPtext_nv[-which(nchar(JPtext_nv$Term, type = "bytes") <= 2),]
JPtext_nv <- JPtext_nv[order(JPtext_nv$Freq, decreasing = T),]
return(JPtext_nv)
}
###############################
# テキストの前処理
###############################
# textall <- video$description
# text90 <- video$description[which(video$pop90==TRUE)]
# textall <- video$tags
# text90 <- video$tags[which(video$pop90==TRUE)]
textall <- video$title
text90 <- video$title[which(video$pop90==FALSE)]
text10 <- video$title[which(video$pop90==TRUE)]
length(textall); length(text90); length(text10)
makeWordFreq <- function(t){
corpus <- makeCorpus(t)
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
# d <- subset(d, d$word != "subscrib") # なんかINの"subscrib"という単語だけなぜか削除できないので力技で消す
# head(d, 10)
d <- d[order(d$freq, decreasing = T),]
rownames(d) <- as.character(c(1:length(d$freq)))
return(d)
}
# ------------
# JP
# ------------
# http://www.ic.daito.ac.jp/~mizutani/mining/rmecab_func.html
wordsall <- make_nounverb_JP(textall)
removeTerms <- c(stopwords("english"), "youtube", "video", "channel", "that",
"show", "watch", "can", "make", "us", "use", "subscribe",
"facebook", "twitter", "instagram", "•", "youtub", "–",
"2017", "2018", "will")
# ------------
# JP
# ------------
# http://www.ic.daito.ac.jp/~mizutani/mining/rmecab_func.html
wordsall <- make_nounverb_JP(textall)
wordsall
words90 <- make_nounverb_JP(text90)
words10 <- make_nounverb_JP(text10)
head(wordsall, 50)
a <- read.table("JPtext.text")
a <- read.table("JPtext.txt")
mode(a)
a[1]
grep("\"", a)
# ------------
# JP用の関数 : 名詞と動詞だけのリスト作成
# ------------
make_nounverb_JP <- function(t){
# for(i in c(length(t))){
#   t[i] <- gsub("\"", "", t[i])
#   t[i] <- gsub("】", "", t[i])
#   t[i] <- gsub("【", "", t[i])
#   t[i] <- gsub("『", "", t[i])
#   t[i] <- gsub("』", "", t[i])
#   t[i] <- gsub("「", "", t[i])
#   t[i] <- gsub("」", "", t[i])
#   t[i] <- gsub("①", "", t[i])
#   t[i] <- gsub("②", "", t[i])
#   t[i] <- gsub("≪", "", t[i])
#   t[i] <- gsub("≫", "", t[i])
#   t[i] <- gsub("→", "", t[i])
#   t[i] <- gsub("年", "", t[i])
#   t[i] <- gsub("月", "", t[i])
#   t[i] <- gsub("日", "", t[i])
#   t[i] <- gsub("！\"", "", t[i])
#   t[i] <- gsub("\"", "", t[i])
#   t[i] <- gsub("〜", "", t[i])
# }
# a <- read.table("JPtext.txt")
# mode(a)
# a[1]
write.table(t, file="JPtext.txt", row.names=F, col.names=F)
JPtextFreq <- RMeCabFreq("JPtext.txt")
JPtextFreq <- rmSign(JPtextFreq)
JPtext_nv <- JPtextFreq[JPtextFreq$Info1 == "名詞" | JPtextFreq$Info1 == "動詞",]
# いらなそうなやつを調べる
table(JPtext_nv$Info1, JPtext_nv$Info2)
subset(JPtext_nv, JPtext_nv$Info1 == "動詞" & JPtext_nv$Info2 == "接尾")
# "がる、させる、す、せる、られる、れる" を削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "動詞" & JPtext_nv$Info2 == "接尾"),]
# "こちら、あっち、あいつ、彼、みなさん"などを削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "名詞" & JPtext_nv$Info2 == "代名詞"),]
# 数字を削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "名詞" & JPtext_nv$Info2 == "数"),]
# 頻出単語をチラ見
JPtext_nv[order(JPtext_nv$Freq, decreasing=T)[1:20],]
# 頻出単語にゴミが混ざってるので手動で削除
removeTerms <- c(removeTerms, "動画", "チャンネル", "登録", "\"", "nn",
"する", "いる", "ある", "なる", "こと", "videos", "さん", "の",
"\"\"", "再生", "リスト", "nnn", "お願い", "！", "゙",
"みる", "〜", "—", "／", "「", "」", "2018", "2017",
"フェイスブック", "ツイッター", "インスタグラム")
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Term %in% removeTerms),]
# "aa"とか"ld"とか"`"とか無意味な単語とか記号を削除
JPtext_nv <- JPtext_nv[-which(nchar(JPtext_nv$Term, type = "bytes") <= 2),]
JPtext_nv <- JPtext_nv[order(JPtext_nv$Freq, decreasing = T),]
return(JPtext_nv)
}
# ------------
# JP
# ------------
# http://www.ic.daito.ac.jp/~mizutani/mining/rmecab_func.html
wordsall <- make_nounverb_JP(textall)
# ------------
# JP用の関数 : 名詞と動詞だけのリスト作成
# ------------
make_nounverb_JP <- function(t){
# for(i in c(length(t))){
#   t[i] <- gsub("\"", "", t[i])
#   t[i] <- gsub("】", "", t[i])
#   t[i] <- gsub("【", "", t[i])
#   t[i] <- gsub("『", "", t[i])
#   t[i] <- gsub("』", "", t[i])
#   t[i] <- gsub("「", "", t[i])
#   t[i] <- gsub("」", "", t[i])
#   t[i] <- gsub("①", "", t[i])
#   t[i] <- gsub("②", "", t[i])
#   t[i] <- gsub("≪", "", t[i])
#   t[i] <- gsub("≫", "", t[i])
#   t[i] <- gsub("→", "", t[i])
#   t[i] <- gsub("年", "", t[i])
#   t[i] <- gsub("月", "", t[i])
#   t[i] <- gsub("日", "", t[i])
#   t[i] <- gsub("！\"", "", t[i])
#   t[i] <- gsub("\"", "", t[i])
#   t[i] <- gsub("〜", "", t[i])
# }
# a <- read.table("JPtext.txt")
# mode(a)
# a[1]
write.table(t, file="JPtext.txt", row.names=F, col.names=F)
JPtextFreq <- RMeCabFreq("JPtext.txt")
# JPtextFreq <- rmSign(JPtextFreq)
JPtext_nv <- JPtextFreq[JPtextFreq$Info1 == "名詞" | JPtextFreq$Info1 == "動詞",]
# いらなそうなやつを調べる
table(JPtext_nv$Info1, JPtext_nv$Info2)
subset(JPtext_nv, JPtext_nv$Info1 == "動詞" & JPtext_nv$Info2 == "接尾")
# "がる、させる、す、せる、られる、れる" を削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "動詞" & JPtext_nv$Info2 == "接尾"),]
# "こちら、あっち、あいつ、彼、みなさん"などを削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "名詞" & JPtext_nv$Info2 == "代名詞"),]
# 数字を削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "名詞" & JPtext_nv$Info2 == "数"),]
# 頻出単語をチラ見
JPtext_nv[order(JPtext_nv$Freq, decreasing=T)[1:20],]
# 頻出単語にゴミが混ざってるので手動で削除
removeTerms <- c(removeTerms, "動画", "チャンネル", "登録", "\"", "nn",
"する", "いる", "ある", "なる", "こと", "videos", "さん", "の",
"\"\"", "再生", "リスト", "nnn", "お願い", "！", "゙",
"みる", "〜", "—", "／", "「", "」", "2018", "2017",
"フェイスブック", "ツイッター", "インスタグラム")
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Term %in% removeTerms),]
# "aa"とか"ld"とか"`"とか無意味な単語とか記号を削除
JPtext_nv <- JPtext_nv[-which(nchar(JPtext_nv$Term, type = "bytes") <= 2),]
JPtext_nv <- JPtext_nv[order(JPtext_nv$Freq, decreasing = T),]
return(JPtext_nv)
}
# ------------
# JP
# ------------
# http://www.ic.daito.ac.jp/~mizutani/mining/rmecab_func.html
wordsall <- make_nounverb_JP(textall)
grep("アイドル", wordsall$Term)
removeTerms
wordsall
