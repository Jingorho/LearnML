library(ggplot2)
library(tm)
library(SnowballC)
library(rpart)
library(rpart.plot)
library("wordcloud")
library("RColorBrewer")
library(RMeCab)
options(scipen=999)
setwd("/Users/yukako/WorkSpace/ML/15_071_AnalyticsEdge/Youtube/data")
USvideo <- read.csv("USvideo_pd.csv")
JPvideo <- read.csv("JPvideo_pd.csv")
INvideo <- read.csv("INvideo_pd.csv")
video <- JPvideo
###############################
# テキストの前処理
###############################
# textall <- video$description
# text90 <- video$description[which(video$pop90==TRUE)]
# textall <- video$tags
# text90 <- video$tags[which(video$pop90==TRUE)]
textall <- video$title
text90 <- video$title[which(video$pop90==FALSE)]
text10 <- video$title[which(video$pop90==TRUE)]
length(textall); length(text90); length(text10)
removeTerms <- c(stopwords("english"), "youtube", "video", "channel", "that",
"show", "watch", "can", "make", "us", "use", "subscribe",
"facebook", "twitter", "instagram", "•", "youtub", "–",
"2017", "2018", "will")
onlywords10 <- data.frame()
formerwords <- ""
formerwords200 <- ""
latterwords <- ""
latterwords200 <- ""
# ------------
# INとUS用の関数 : コーパスを作る
# ------------
makeCorpus <- function(t){
# なぜか削除できない単語を無理やり削除
t <- gsub("–", "", t)
t <- gsub("video", "", t)
t <- gsub("subscribe", "", t)
corpus = Corpus(VectorSource(t)) # Warning出るけどVCorpusにするとrpartできない
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, removeTerms) # ほかあれば追加
corpus = tm_map(corpus, stemDocument)
return (corpus)
}
makeDocumentTerms_JP <- function(dontMakeFile, t){
if(!dontMakeFile){
dir.create("JPtexts")
for(i in c(1:length(t))){
fname <- paste("JPtexts/", i, ".txt", sep="")
write.table(t[i], file=fname, row.names=F, col.names=F)
}
}
dt <- docMatrix("JPtexts",
pos=c("名詞", "動詞", "形容詞")) # MeCabの関数
dt <- dt[rowSums(dt) >= 2,] # 全文書を通しての総頻度が 2 以上のターム
dt_transpose_df <- data.frame(t(matrix(as.matrix(dt), nrow(dt), ncol(dt))))
rownames(dt_transpose_df) <- colnames(dt)
colnames(dt_transpose_df) <- rownames(dt)
dt <- as.data.frame(dt_transpose_df)
dt <- dt[,-c(1,2)] # 最初の2列は集計なので削除
str(dt)
dim(dt)
removedFile <- c()
for(i in c(1:length(t))){
if(sum(rownames(dt) %in% paste(i, ".txt", sep="")) == 0){
removedFile <- c(removedFile, i)
}
}
return(dt)
}
fileList <- list.files("JPtexts")
fileList
file.remove(fileList)
fileList <- list.files("JPtexts")
fileList
apply(fileList, 1, file.remove)
file.remove(fileList)
warnings()
file.remove(paste("JPtexts", fileList, sep=""))
paste("JPtexts", fileList, sep="")
file.remove(paste("JPtexts/", fileList, sep=""))
file.remove("JPtexts")
fileList <- list.files("JPtexts")
file.remove(paste("JPtexts/", fileList, sep=""))
fileList
file.remove("JPtexts")
dir.create("JPtexts")
makeDocumentTerms_JP <- function(dontMakeFile, t){
if(!dontMakeFile){
fileList <- list.files("JPtexts")
file.remove(paste("JPtexts/", fileList, sep=""))
dir.create("JPtexts")
for(i in c(1:length(t))){
fname <- paste("JPtexts/", i, ".txt", sep="")
write.table(t[i], file=fname, row.names=F, col.names=F)
}
}
dt <- docMatrix("JPtexts",
pos=c("名詞", "動詞", "形容詞")) # MeCabの関数
dt <- dt[rowSums(dt) >= 2,] # 全文書を通しての総頻度が 2 以上のターム
dt_transpose_df <- data.frame(t(matrix(as.matrix(dt), nrow(dt), ncol(dt))))
rownames(dt_transpose_df) <- colnames(dt)
colnames(dt_transpose_df) <- rownames(dt)
dt <- as.data.frame(dt_transpose_df)
dt <- dt[,-c(1,2)] # 最初の2列は集計なので削除
str(dt)
dim(dt)
removedFile <- c()
for(i in c(1:length(t))){
if(sum(rownames(dt) %in% paste(i, ".txt", sep="")) == 0){
removedFile <- c(removedFile, i)
}
}
return(dt)
}
docterm <- makeDocumentTerms_JP(F, text90)
docterm <- makeDocumentTerms_JP(F, text10)
dim(docterm)
# ------------
sum(video$pop90)
removedFile
removedFile <- c()
docterm <- makeDocumentTerms_JP(T, text10)
dim(docterm)
v <- video[video$pop90,]
v <- v[-removedFile,]
removedFile
removedFile
removedFile <- c()
# head(document_terms)
# rownames(docterm) <- c(1:dim(docterm)[1])
# head(docterm[1:10,1:10])
t <- as.character(text10)
dt
str(dt)
dim(dt)
dt <- docMatrix("JPtexts",
pos=c("名詞", "動詞", "形容詞")) # MeCabの関数
dt <- dt[rowSums(dt) >= 2,] # 全文書を通しての総頻度が 2 以上のタームのみ残す
dt_transpose_df <- data.frame(t(matrix(as.matrix(dt), nrow(dt), ncol(dt))))
rownames(dt_transpose_df) <- colnames(dt)
colnames(dt_transpose_df) <- rownames(dt)
dt <- as.data.frame(dt_transpose_df)
dt <- dt[,-c(1,2)] # 最初の2列は集計なので削除
str(dt)
dim(dt)
for(i in c(1:length(t))){
if(sum(rownames(dt) %in% paste(i, ".txt", sep="")) == 0){
removedFile <- c(removedFile, i)
}
}
removedFile
v <- v[-removedFile,]
v
# 目的変数はpopXX
colnames(v)
summary(v$descLen)
docterm$pop90 = (v$descLen < 100)
# Reciataionでは日付を元にsplitしてたけど、今回は単にランダムに6:4でtrain:test
split1 = sample(row.names(docterm), 0.6*nrow(docterm))
split2 = setdiff(row.names(docterm), split1)
train = docterm[split1,]
test = docterm[split2,]
cart = rpart(pop90 ~ ., data = train, method = "class", cp = .00000003)
prp(cart, cex=0.8)
cart = rpart(pop90 ~ ., data = docterm, method = "class", cp = .00000003)
prp(cart, cex=0.8)
par(family = "HiraKakuProN-W3")
cart = rpart(pop90 ~ ., data = docterm, method = "class", cp = .00000003)
prp(cart, cex=0.8)
str(dt)
