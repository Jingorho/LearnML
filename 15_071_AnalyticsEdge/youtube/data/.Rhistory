library(ggplot2)
library(tm)
library(SnowballC)
library(rpart)
library(rpart.plot)
library("wordcloud")
library("RColorBrewer")
library(RMeCab)
options(scipen=999)
setwd("/Users/yukako/WorkSpace/ML/15_071_AnalyticsEdge/Youtube/data")
JPvideo <- read.csv("JPvideo_pd.csv")
# video <- USvideo
video <- JPvideo
selectedVideoId <- which(video$pop==TRUE)
analyzedText <- tolower(video$description[selectedVideoId])
write.table(analyzedText, file="JPtext.txt", row.names=F, col.names=F)
JPtextFreq <- RMeCabFreq("JPtext.txt")
JPtext_nv <- JPtextFreq[JPtextFreq$Info1 == "名詞" | JPtextFreq$Info1 == "動詞",]
# "がる、させる、す、せる、られる、れる" を削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "動詞" & JPtext_nv$Info2 == "接尾"),]
# "こちら、あっち、あいつ、彼、みなさん"などを削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "名詞" & JPtext_nv$Info2 == "代名詞"),]
# 数字を削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "名詞" & JPtext_nv$Info2 == "数"),]
# 頻出単語をチラ見
JPtext_nv[order(JPtext_nv$Freq, decreasing=T)[1:20],]
# 頻出単語にゴミが混ざってるので手動で削除
removeTerms <- c(removeTerms, "動画", "チャンネル", "登録", "\"", "n", "nn",
"する", "いる", "ある", "なる", "こと", "videos", "さん", "の",
"\"\"")
###############################
# 処理をまとめて関数にした
# 一回読み込んだら、中身を編集しない限り、繰り返し使えます。
# 使用例:
# 分析結果のドキュメント <- makeDocumentTerms(分析するテキスト, 抽出する単語の頻出度(0.90なら上位90%))
###############################
removeTerms <- c(stopwords("english"), "youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe", "us", "use")
# 頻出単語にゴミが混ざってるので手動で削除
removeTerms <- c(removeTerms, "動画", "チャンネル", "登録", "\"", "n", "nn",
"する", "いる", "ある", "なる", "こと", "videos", "さん", "の",
"\"\"")
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Term %in% removeTerms),]
nchar(JPtext_nv$Term[1], type = "bytes")
JPtext_nv$Term[1]
nchar(JPtext_nv$Term[2], type = "bytes")
JPtext_nv$Term[2]
JPtext_nv$Term[4]
nchar(JPtext_nv$Term[4], type = "bytes")
nchar(d, type = "bytes")
nchar("d", type = "bytes")
nchar(JPtext_nv$Term[1:10], type = "bytes")
JPtext_nv[which(nchar(JPtext_nv$Term, type = "bytes") == 2)]
JPtext_nv[which(nchar(JPtext_nv$Term, type = "bytes") == 2),]
# "aa"とか"ld"とか"`"とか無意味な単語とか記号を削除
JPtext_nv <- JPtext_nv[-which(nchar(JPtext_nv$Term, type = "bytes") <= 2),]
# 頻出単語をチラ見
JPtext_nv[order(JPtext_nv$Freq, decreasing=T)[1:20],]
par(family = "Osaka") # 日本語フォント使えるようにする設定。WindowsとMacで違うかも
wordcloud(JPtext_nv$Term, JPtext_nv$Freq,
color=rainbow(5), random.order=F, random.color=F)
selectedVideo <- video$description[which(video$pop==TRUE)]
selectedVideo <- video$description
analyzedText <- tolower(selectedVideo)
write.table(analyzedText, file="JPtext.txt", row.names=F, col.names=F)
JPtextFreq <- RMeCabFreq("JPtext.txt")
JPtext_nv <- JPtextFreq[JPtextFreq$Info1 == "名詞" | JPtextFreq$Info1 == "動詞",]
# いらなそうなやつを調べる
table(JPtext_nv$Info1, JPtext_nv$Info2)
subset(JPtext_nv, JPtext_nv$Info1 == "動詞" & JPtext_nv$Info2 == "接尾")
# "がる、させる、す、せる、られる、れる" を削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "動詞" & JPtext_nv$Info2 == "接尾"),]
# "こちら、あっち、あいつ、彼、みなさん"などを削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "名詞" & JPtext_nv$Info2 == "代名詞"),]
# 数字を削除
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Info1 == "名詞" & JPtext_nv$Info2 == "数"),]
# 頻出単語をチラ見
JPtext_nv[order(JPtext_nv$Freq, decreasing=T)[1:20],]
# 頻出単語にゴミが混ざってるので手動で削除
removeTerms <- c(removeTerms, "動画", "チャンネル", "登録", "\"", "n", "nn",
"する", "いる", "ある", "なる", "こと", "videos", "さん", "の",
"\"\"", "再生", "リスト")
JPtext_nv <- JPtext_nv[-which(JPtext_nv$Term %in% removeTerms),]
# "aa"とか"ld"とか"`"とか無意味な単語とか記号を削除
JPtext_nv <- JPtext_nv[-which(nchar(JPtext_nv$Term, type = "bytes") <= 2),]
# 頻出単語をチラ見
JPtext_nv[order(JPtext_nv$Freq, decreasing=T)[1:20],]
par(family = "Osaka") # 日本語フォント使えるようにする設定。WindowsとMacで違うかも
wordcloud(JPtext_nv$Term, JPtext_nv$Freq,
color=rainbow(5), random.order=F, random.color=F)
makeDocumentTerms <- function(analyzedText, atleaset_percentage){
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
findFreqTerms(frequencies, lowfreq=500)
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms); dim(document_terms)
# head(document_terms)
return (document_terms)
}
makeDocumentTerms <- function(analyzedText, atleaset_percentage){
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
return (corpus)
}
makeCorpus <- function(analyzedText, atleaset_percentage){
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
return (corpus)
}
###############################
# 実際にテキストデータに対して分析処理を行う
###############################
# month <- 6
# text <- subset(as.character(video$description), video$publish_month==month)
video$description <- gsub("video", "", video$description) # なぜかremoveWordsで"video"がremoveできないので無理やり削除
text <- as.character(video$description)
corpus <- makeCorpus(text, 0.90) # 0.99がrecitationのデフォ
frequencies = DocumentTermMatrix(corpus)
frequencies
findFreqTerms(frequencies, lowfreq=1000)
findFreqTerms(frequencies, lowfreq=500)
USvideo <- read.csv("USvideo_pd.csv")
video <- USvideo
###############################
# 実際にテキストデータに対して分析処理を行う
###############################
# month <- 6
# text <- subset(as.character(video$description), video$publish_month==month)
video$description <- gsub("video", "", video$description) # なぜかremoveWordsで"video"がremoveできないので無理やり削除
text <- as.character(video$description)
corpus <- makeCorpus(text, 0.90) # 0.99がrecitationのデフォ
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
findFreqTerms(frequencies, lowfreq=500)
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
atleaset_percentage <- 0.90
makeCorpus <- function(analyzedText){
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
return (corpus)
}
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms); dim(document_terms)
sparse
dtm <- TermDocumentMatrix(corpus)
dtm
frequencies
m <- as.matrix(dtm)
dim(m)
m[1]
m[1,2]
m[1,4]
m[1:5,1:5]
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v), freq = v)
head(d, 10)
makeCorpus <- function(analyzedText){
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords, removeTerms) # ほかあれば
corpus = tm_map(corpus, stemDocument)
return (corpus)
}
head(d, 10)
# ワードクラウド描画!
wordcloud(d$word, d$freq,
color=rainbow(5), random.order=F, random.color=F)
selectedVideo <- video$description[which(video$pop==TRUE)]
# selectedVideo <- video$description
text <- as.character(selectedVideo)
atleaset_percentage <- 0.90
corpus <- makeCorpus(text) # 0.99がrecitationのデフォ
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
findFreqTerms(frequencies, lowfreq=500)
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms); dim(document_terms)
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
head(d, 10)
# ワードクラウド描画!
wordcloud(d$word, d$freq,
color=rainbow(5), random.order=F, random.color=F)
INvideo <- read.csv("INvideo_pd.csv")
# video <- USvideo
# video <- JPvideo
video <- INvideo
# video <- USvideo
# video <- JPvideo
video <- INvideo
###############################
# 実際にテキストデータに対して分析処理を行う
###############################
# month <- 6
# text <- subset(as.character(video$description), video$publish_month==month)
video$description <- gsub("video", "", video$description) # なぜかremoveWordsで"video"がremoveできないので無理やり削除
selectedVideo <- video$description
text <- as.character(selectedVideo)
atleaset_percentage <- 0.90
corpus <- makeCorpus(text) # 0.99がrecitationのデフォ
frequencies = DocumentTermMatrix(corpus)
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms); dim(document_terms)
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
head(d, 10)
# ワードクラウド描画!
wordcloud(d$word, d$freq,
color=rainbow(5), random.order=F, random.color=F)
selectedVideo <- video$description[which(video$pop==TRUE)]
text <- as.character(selectedVideo)
atleaset_percentage <- 0.90
corpus <- makeCorpus(text) # 0.99がrecitationのデフォ
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
findFreqTerms(frequencies, lowfreq=500)
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms); dim(document_terms)
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word = names(v), freq = v)
head(d, 10)
# ワードクラウド描画!
wordcloud(d$word, d$freq,
color=rainbow(5), random.order=F, random.color=F)
