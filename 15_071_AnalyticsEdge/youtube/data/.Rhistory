corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeWords, c("youtube", "video", "channel", "that")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
strwrap(corpus[[1]])
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
findFreqTerms(frequencies, lowfreq=500)
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms); dim(document_terms)
# head(document_terms)
return (document_terms)
}
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
library(tm)
library(SnowballC)
library(rpart)
library(rpart.plot)
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
analyzedText <- text
corpus = Corpus(VectorSource(analyzedText))
text <- subset(as.character(video$description), video$publish_month==month)
analyzedText <- text
corpus = Corpus(VectorSource(analyzedText))
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
str(document_terms); dim(document_terms)
# 目的変数はpop(めちゃ人気かどうか)
document_terms$pop = subset(video$pop, video$publish_month == month)
# Reciataionでは日付を元にsplitしてたけど、今回は単にランダムに6:4でtrain:test
split1 = sample(row.names(document_terms), 0.6*nrow(document_terms))
split2 = setdiff(row.names(document_terms), split1)
train = document_terms[split1,]
test = document_terms[split2,]
cart = rpart(superPop ~ ., data=train, method="class", cp = .003)
cart = rpart(pop ~ ., data=train, method="class", cp = .003)
prp(cart, cex=0.6)
makeDocumentTerms <- function(analyzedText, atleaset_percentage){
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeWords,
c("youtube", "video", "channel", "that",
"show", "watch")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
strwrap(corpus[[1]])
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
findFreqTerms(frequencies, lowfreq=500)
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms); dim(document_terms)
# head(document_terms)
return (document_terms)
}
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
# 目的変数はpop(めちゃ人気かどうか)
document_terms$pop = subset(video$pop, video$publish_month == month)
# Reciataionでは日付を元にsplitしてたけど、今回は単にランダムに6:4でtrain:test
split1 = sample(row.names(document_terms), 0.6*nrow(document_terms))
split2 = setdiff(row.names(document_terms), split1)
train = document_terms[split1,]
test = document_terms[split2,]
cart = rpart(pop ~ ., data=train, method="class", cp = .003)
prp(cart, cex=0.6)
removeWords
stopwords("english")
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeWords,
c("youtube", "video", "channel", "that",
"show", "watch", "can", "make")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
strwrap(corpus[[1]])
frequencies = DocumentTermMatrix(corpus)
frequencies
findFreqTerms(frequencies, lowfreq=1000)
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords, stopwords("english"))
# corpus = tm_map(corpus, removeWords,
#                 c("youtube", "video", "channel", "that",
#                   "show", "watch", "can", "make")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeWords,
c("youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscrib")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
strwrap(corpus[[1]])
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
findFreqTerms(frequencies, lowfreq=500)
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
document_terms = as.data.frame(as.matrix(sparse))
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, removeWords,
c("youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscrib")) # ほかあれば
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
corpus = tm_map(corpus, removeWords,
c("youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscrib")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removeWords,
c("youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscrib")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
# 時間、日時関係のいろいろをやってみたスクリプト。
old.packages()
# 時間、日時関係のいろいろをやってみたスクリプト。
update.packages()
update.packages(ask = FALSE)
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
corpus = VCorpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that", "like",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
analyzedText <- gsub("video", "", analyzedText)
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
# 目的変数はpop(めちゃ人気かどうか)
document_terms$pop = subset(video$pop, video$publish_month == month)
document_terms$pop
document_terms
makeDocumentTerms <- function(analyzedText, atleaset_percentage){
# なぜかremoveWordsで"video"がremoveできないので無理やり削除
analyzedText <- gsub("video", "", analyzedText)
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
findFreqTerms(frequencies, lowfreq=500)
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms); dim(document_terms)
# head(document_terms)
return (document_terms)
}
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
# 目的変数はpop(めちゃ人気かどうか)
document_terms$pop = subset(video$pop, video$publish_month == month)
document_terms
# Reciataionでは日付を元にsplitしてたけど、今回は単にランダムに6:4でtrain:test
split1 = sample(row.names(document_terms), 0.6*nrow(document_terms))
split2 = setdiff(row.names(document_terms), split1)
train = document_terms[split1,]
test = document_terms[split2,]
cart = rpart(pop ~ ., data=train, method="class", cp = .003)
prp(cart, cex=0.6)
video$description.count("offici")
grep(video$description, "offici")
video$description[1]
grep(as.character(video$description[1]), "")
as.character(video$description[1])
grep(as.character(video$description[1]), "drone")
charmatch(as.character(video$description[1]), "drone")
grep("drone", as.character(video$description[1]))
grep("drone", as.character(video$description))
grep("offici", as.character(video$description))
# video <- JPvideo
# video <- INvideo
head(video)
a <- subset(video$description, video$pop==1)
grep("offici", a)
regexpr("offici", a)
grep("offici", a)
a[7]
i<-1
grep("offici", a[i])
popvideodes <- subset(video$description, video$pop==1)
grep("offici", popvideodes)
idd <- grep("offici", popvideodes)
popvideodes[idd]
popvideodes <- grep("offici", subset(video$description, video$pop==1))
popvideodes
popvideodes <- video$description[grep("offici", subset(video$description, video$pop==1))]
popvideodes
length(popvideodes)
grep("offici", popvideodes)
length(popvideodes)
grep("offici", popvideodes[1])
length(popvideodes)
length(popvideodes)
a <- c("ddd", "aaa", "lll", "gaga")
grep("a", a)
grep("a", a)
a <- c("ddda", "aaa", "lll", "gaga")
grep("a", a)
grep("offici", subset(video$description, video$pop==1))
video$description[14]
grep("offici", subset(video$description, video$pop==1))
video$description[7]
grep("offici", subset(video$description, video$pop==1))
write.csv("text.csv", subset(video$description, video$pop==1))
write.csv(subset(video$description, video$pop==1), "text.csv")
prp(cart, cex=0.8)
month <- 1
text <- subset(as.character(video$description), video$publish_month==month)
makeDocumentTerms <- function(analyzedText, atleaset_percentage){
# なぜかremoveWordsで"video"がremoveできないので無理やり削除
analyzedText <- gsub("video", "", analyzedText)
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
findFreqTerms(frequencies, lowfreq=500)
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms); dim(document_terms)
# head(document_terms)
return (document_terms)
}
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
# 目的変数はpop(めちゃ人気かどうか)
document_terms$pop = subset(video$pop, video$publish_month == month)
# Reciataionでは日付を元にsplitしてたけど、今回は単にランダムに6:4でtrain:test
split1 = sample(row.names(document_terms), 0.6*nrow(document_terms))
split2 = setdiff(row.names(document_terms), split1)
train = document_terms[split1,]
test = document_terms[split2,]
cart = rpart(pop ~ ., data=train, method="class", cp = .003)
prp(cart, cex=0.8)
month <- 6
text <- subset(as.character(video$description), video$publish_month==month)
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
# 目的変数はpop(めちゃ人気かどうか)
document_terms$pop = subset(video$pop, video$publish_month == month)
# Reciataionでは日付を元にsplitしてたけど、今回は単にランダムに6:4でtrain:test
split1 = sample(row.names(document_terms), 0.6*nrow(document_terms))
split2 = setdiff(row.names(document_terms), split1)
train = document_terms[split1,]
test = document_terms[split2,]
cart = rpart(pop ~ ., data=train, method="class", cp = .003)
prp(cart, cex=0.8)
# month <- 6
# text <- subset(as.character(video$description), video$publish_month==month)
text <- as.character(video$description)
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
# 目的変数はpop(めちゃ人気かどうか)
document_terms$pop = subset(video$pop, video$publish_month == month)
# 目的変数はpop(めちゃ人気かどうか)
document_terms$pop = video$pop
# Reciataionでは日付を元にsplitしてたけど、今回は単にランダムに6:4でtrain:test
split1 = sample(row.names(document_terms), 0.6*nrow(document_terms))
split2 = setdiff(row.names(document_terms), split1)
train = document_terms[split1,]
test = document_terms[split2,]
cart = rpart(pop ~ ., data=train, method="class", cp = .003)
prp(cart, cex=0.8)
# video <- USvideo
video <- JPvideo
###############################
# publish_time時刻との閲覧数の関係 -> 結論: Youtuberは1-2時間前倒しでpublishするべき?
###############################
# 日付を削って時刻だけ取り出して分布をみる
hist(video$publish_hour, breaks=24)
###############################
# publish_time時刻との閲覧数の関係 -> 結論: Youtuberは1-2時間前倒しでpublishするべき?
###############################
# 日付を削って時刻だけ取り出して分布をみる
# hist(video$publish_hour, breaks=24)
# 時刻と閲覧数の関係を見たい
hours_views <- c(0:23)
for(i in c(0:23)){
# 0-23時に公開された動画を抽出して、そのviewをsum()
hours_views[i-1] <- sum(as.numeric(subset(video, video$publish_hour==i)$views))
}
# データフレーム にまとめる
hour_views <- data.frame(
table(video$publish_hour), # 0-23時に公開された動画の本数
hours_views) # その時間に公開された動画の閲覧数(その時刻での閲覧数じゃない)
colnames(hour_views) <- c("hours", "counts", "views")
# 色の濃さは、その時間に公開された動画の閲覧数
g <- ggplot(hour_views, aes(x = hour_views$hours, y = hour_views$counts, fill = hour_views$views))
g <- g + geom_bar(stat = "identity")
g <- g + ggtitle("Publish hour and views") + xlab("Publish hour") + ylab("Frequency")
plot(g) # なんか2時に公開された動画がやたら見られてる。。?
###############################
# publish_time日付と人気の関係
###############################
# 2017-10以降にデータが集中してる...なんで？データに変に偏りないかな。不安
table(format(as.POSIXct(video$publish_time), "%Y-%m"))
video$publish_month <- format(as.POSIXct(video$publish_time), "%m")
video$publish_month <- as.numeric(video$publish_month)
hist(video$publish_month, breaks=12) # 11~5月に集中
# month <- 6
# text <- subset(as.character(video$description), video$publish_month==month)
text <- as.character(video$description)
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
# 目的変数はpop(めちゃ人気かどうか)
document_terms$pop = video$pop
# Reciataionでは日付を元にsplitしてたけど、今回は単にランダムに6:4でtrain:test
split1 = sample(row.names(document_terms), 0.6*nrow(document_terms))
split2 = setdiff(row.names(document_terms), split1)
train = document_terms[split1,]
test = document_terms[split2,]
cart = rpart(pop ~ ., data=train, method="class", cp = .003)
cart = rpart(pop ~ ., data=train, method="class", cp = .003)
test
cart = rpart(pop ~ ., data=train, method="class", cp = .003)
train = document_terms[split1,]
test = document_terms[split2,]
cart = rpart(pop ~ ., data=train, method="class", cp = .003)
# Reciataionでは日付を元にsplitしてたけど、今回は単にランダムに6:4でtrain:test
split1 = sample(row.names(document_terms), 0.6*nrow(document_terms))
split2 = setdiff(row.names(document_terms), split1)
train = document_terms[split1,]
test = document_terms[split2,]
document_terms
head(document_terms)
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
head(text)
# video <- USvideo
# video <- JPvideo
video <- INvideo
###############################
# publish_time時刻との閲覧数の関係 -> 結論: Youtuberは1-2時間前倒しでpublishするべき?
###############################
# 日付を削って時刻だけ取り出して分布をみる
# hist(video$publish_hour, breaks=24)
# 時刻と閲覧数の関係を見たい
hours_views <- c(0:23)
for(i in c(0:23)){
# 0-23時に公開された動画を抽出して、そのviewをsum()
hours_views[i-1] <- sum(as.numeric(subset(video, video$publish_hour==i)$views))
}
# データフレーム にまとめる
hour_views <- data.frame(
table(video$publish_hour), # 0-23時に公開された動画の本数
hours_views) # その時間に公開された動画の閲覧数(その時刻での閲覧数じゃない)
colnames(hour_views) <- c("hours", "counts", "views")
# 色の濃さは、その時間に公開された動画の閲覧数
g <- ggplot(hour_views, aes(x = hour_views$hours, y = hour_views$counts, fill = hour_views$views))
g <- g + geom_bar(stat = "identity")
g <- g + ggtitle("Publish hour and views") + xlab("Publish hour") + ylab("Frequency")
plot(g) # なんか2時に公開された動画がやたら見られてる。。?
###############################
# publish_time日付と人気の関係
###############################
# 2017-10以降にデータが集中してる...なんで？データに変に偏りないかな。不安
table(format(as.POSIXct(video$publish_time), "%Y-%m"))
video$publish_month <- format(as.POSIXct(video$publish_time), "%m")
video$publish_month <- as.numeric(video$publish_month)
hist(video$publish_month, breaks=12) # 11~5月に集中
# month <- 6
# text <- subset(as.character(video$description), video$publish_month==month)
text <- as.character(video$description)
head(text)
makeDocumentTerms <- function(analyzedText, atleaset_percentage){
# なぜかremoveWordsで"video"がremoveできないので無理やり削除
analyzedText <- gsub("video", "", analyzedText)
corpus = Corpus(VectorSource(analyzedText))
corpus = tm_map(corpus, tolower)
if (!("PlainTextDocument" %in% class(corpus[[1]]))) {
corpus = tm_map(corpus, PlainTextDocument)
}
corpus = tm_map(corpus, removeWords,
c(stopwords("english"),
"youtube", "video", "channel", "that",
"show", "watch", "can", "make", "subscribe")) # ほかあれば
corpus = tm_map(corpus, stemDocument)
frequencies = DocumentTermMatrix(corpus)
findFreqTerms(frequencies, lowfreq=1000)
findFreqTerms(frequencies, lowfreq=500)
sparse = removeSparseTerms(frequencies, atleaset_percentage); dim(sparse)
document_terms = as.data.frame(as.matrix(sparse))
str(document_terms); dim(document_terms)
# head(document_terms)
return (document_terms)
}
document_terms <- makeDocumentTerms(text, 0.90) # 0.99がrecitationのデフォ
# 目的変数はpop(めちゃ人気かどうか)
document_terms$pop = video$pop
# Reciataionでは日付を元にsplitしてたけど、今回は単にランダムに6:4でtrain:test
split1 = sample(row.names(document_terms), 0.6*nrow(document_terms))
split2 = setdiff(row.names(document_terms), split1)
train = document_terms[split1,]
test = document_terms[split2,]
cart = rpart(pop ~ ., data=train, method="class", cp = .003)
prp(cart, cex=0.8)
# video <- USvideo
video <- JPvideo
library(RMeCab)
install.packages("RMeCab")
library(RMeCab)
install.packages("wordcloud") # word-cloud generator
install.packages("RColorBrewer") # color palettes
library("wordcloud")
library("RColorBrewer")
# https://exploratory.io/note/2ac8ae888097/Mecab-RMeCab-0944283373151109
install.packages("RMeCab", repos = "http://rmecab.jp/R")
library(RMeCab)
library("wordcloud")
library("RColorBrewer")
RMeCabFreq(text)
